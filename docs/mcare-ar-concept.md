A Mass Casualty Simulation Game for Eergencies
Nelson Olim M.D., Peter Smee, Ph.D., David Chandross, Ph.D., Raquel Meyer, Ph.D., RN

Concept Document

This document describes the development of a mass casualty training game system for release in late May, 2021. 

The Challenge

Emergency departments in hospitals are traditionally not prepared to receive mass casualties. The state of preparation for such events is a strong predictor of survival outcome. The goal is to design an augmented reality table-top simulation drill in order to enable teams to achieve a state of effective preparation. In reality, it is very hard for teams to stop an emergency unit from running for a period of ‘shutdown’ to practice drills. 

The goal of this game is to save lives by preventing ER departments from being overwhelmed by the cognitive and physical load associated with a mass casualty scenario by using immersive, on-demand training. This gamified simulation system will be scalable to reach a world-wide audience and will feature augmented reality collaborative learning. By supporting the development of this instrument, Facebook would enhance connectivity by increasing accessibility of learning to strengthen training and performance of medical systems, with and without resources, in diverse countries across the globe to contribute to health outcome equity. This solution will enable healthcare teams to increase local capacity and response to mass casualties by creating tailored learning reflective of their local conditions and resources.

The Technology

Through the use of LIDAR and 360 camera mapping, we will reproduce the users’ emergency care settings in augmented reality that permit a user to run a simulation, save progress, repeat the simulation, monitor progression and attempt new strategies with feedback and reflective loops. Through the use of a hand recognition enabled interface, users will be able to intuitively interact with others by making clinical incident command decisions. This will be designed not as a pure simulation, but as a serious game with many elements of gamification deployed to increase engagement and motivate teams to increase their score by improving patient flow and management. 

The game intelligence will permit not only a simulation of virtual patient progression, but also randomized presentation of different numbers of patients entering the ER after a mass casualty event. The code base dictates a specific ‘life path’ for each virtual patient and progress in the game system will be based on increasing the total number of minutes of life added to each one. A simulated patient who undergoes several stages of complication will have scores assigned to the management of each phase with real-world based die roll probability ‘hits’ for treatment success. The code for the simulation will be designed to increased challenge difficulty in response to achieving good results with easier incident rehearsals.

In the form of a simulation and advanced gaming environment (SAGE), progression feedback is granular and capable of being monitored through the game engine back end. The back end interface for faculty or other training providers (WHO Academy Staff or subject matter experts) will be self-authoring, in that new content can be continuously uploaded. This will permit ‘just-in-time’ responsive training as well, given that certain simulation conditions will overwhelm the response team playing the game and opportunities to provide more practice at that element need to be available. 

Technology Deployment and Simulation Modulation

This will consist of a ‘field kit’ which includes a movable camera mount, a 360 camera, a LIDAR device and simulation game software. This will feature a high quality heads up display (HUD) that can be navigated by new teams easily and permit saving of progress in a distributed server. The actual game system will be downloaded by each user for speed and simplicity of implementation. Our goal is not only to produce an effective solution for this challenge, but also to show the WHO Academy how to design immersive learning experiences for future trend setting in this space. 


Gamified Simulation

The blending of game-based learning and simulation is critical and is a deliberate strategy based on the rich meta-analyses of this educational tool across the literature. Simulation literature is well established and its pairing with game design provides the user with a rich sensory and emotive experience that helps learners practice and retain new skills. Simulation intensity can be modified with respect to severity of cases, numbers of cases managed per unit time and through gradually being able to admit more and more patients in a tight time window and improve outcomes. 

The game logic will use the prime metric of total minutes of life saved across all virtual patients during any given ‘session’. The progress from this session will be auto-saved or manually paused to permit the team to return to identical conditions if they wish for later play. There are a number of game modes including:

1. Walk-through simulation - The users familiarize themselves with the simulated environment and learn the game features.
2. Free form simulation - A simulator interface will permit teams to increase patients per time, the types of patients that arrive and other conditions such as number of doctors, nurses or available beds.
3. Gamified progressive simulation - Teams will take on increasingly more difficult conditions with the opportunity to freeze game progress and communicate about solutions, real-time through the game interface.
4. Push to destruction - The game intensifies conditions until the team fails according to defined criteria.
5. On demand training - The instructor can add new cases to the simulation set if they see that a team is having difficulty with a particular type of case. 

As such, this provides multiple simulation modes from realistic to high challenge using an intuitive interface and a low cost deployment strategy. 

The interface will be attractive and easy to use with hand navigation and some desk top or device extensions permitting players to set simulation parameters or replay critical incidents. The main game mechanic is the logging of progress as the team builds capacity to manage mass casualties.

Deployment 

The first face to face trial will begin in June, the production date for rollout is the end of May, 2021. The plan is to gradually roll out the game system and build distribution capacity after user input. The goal is to have this in use broadly by the end of 2021. 

Design Documentation

There are 3 Phases of design. 

Phase I: Ideation

In this phase case cards will be sorted into scoring systems and designed for titrated challenge for players. The augmented reality block emergency unit simulation will be detailed and technical specifications documentation will be completed for the design vendor. This includes a description of how simulated patients will flow through the system, how their life paths and outcomes will be recorded and how player actions will influence this flow. A technical specification document will be produced to provide the programming design team with data for the first build. Translation and accessibility options will be introduced.

Card and Challenge Titration Specifications

During Phase I each case card will be assigned a scoring value and integrated with rate of patient flow adjustment. Levels or other indicators of progress will be designed to provide analytics for players. Graphic design concept art will be specified for the artist team to begin work in Phase II.

Concept Art and Hedonic Design

During Phase I concept art will be generated by the team in preparation for bringing the artist team on board. The player interface will be designed and integrated with analytics at the game master back end. This will culminate in a design document listing resources needed to build the game.

Augmented Reality Block Design

During Phase I the procedural flow of augmented reality use by players will be specified. 

Technical Specifications

During Phase I the team will produce a Technical Specifications document for use by prospective vendors to develop a budget for the final build. This must meet technical guidelines established by the W.H.O. and will need to incorporate any platform compatibility issues that are projected.

Phase II: Development 

In this phase the programming design team will work with the game design unit to produce the prototype AR experience and integrate the vector 2D graphics engine with learning code used for scoring and analytics. This design will be submitted for an alpha test version of the product.

Augmented Reality Block Build

During Phase II the selected vendor will proceed to develop the AR block system and designate the device and platform used to generate the experience through untethered systems. The AR block system will be integrated into the overall code for gameplay.

3D Vector Graphics Build

This is divided into Phase II art selection and integration with code for analytics and game progression. The vendor will produce artwork and this will integrate with in-game actions describing patient flow dynamics, control of experience intensity and scoring systems.


Phase III: Testing and Beta Release 

Feedback from alpha testing will be utilized to build the final game iteration including a splash launch screen, integration with LMS portals and player token management in the system. 

Alpha Testing

During Phase III, workable prototype functional units in the game will be tested for ease of use, fidelity of simulation and UX interface build. 

Beta Testing

An early player test version of the game will be released on a Public Test Realm (PTR) During Phase III for invited guests to provide feedback on game experience.

Final Iteration Release

The release of build version 1.x will be produced and released with a 6 month window of feedback data collection for build 2.x intended for wider distribution. 






